{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bac60efe",
        "outputId": "0cf0183c-d7c1-4e3e-9278-5d56b292fb9a"
      },
      "source": [
        "pip install --quiet wikipedia sentence-transformers faiss-cpu transformers torch langchain_community"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff52c81b"
      },
      "source": [
        "**Reasoning**:\n",
        "The required libraries have been installed. Now, I will import the necessary classes and load the MiniLM embedding model and FLAN-T5-small tokenizer and model as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b977c981",
        "outputId": "2cbb838e-0aad-4429-e286-f5ac76062965"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load MiniLM embedding model\n",
        "minilm_model = SentenceTransformer(minilm_model_name)\n",
        "print(f\"MiniLM embedding model '{minilm_model_name}' loaded successfully.\")\n",
        "\n",
        "# Load FLAN-T5-small tokenizer and model\n",
        "flan_tokenizer = AutoTokenizer.from_pretrained(flan_model_name)\n",
        "flan_model = AutoModelForSeq2SeqLM.from_pretrained(flan_model_name)\n",
        "print(f\"FLAN-T5-small model '{flan_model_name}' and tokenizer loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MiniLM embedding model 'all-MiniLM-L6-v2' loaded successfully.\n",
            "FLAN-T5-small model 'google/flan-t5-small' and tokenizer loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "466df9d0",
        "outputId": "050133dd-c5b6-4250-c7f0-dda07c4d55db"
      },
      "source": [
        "import wikipedia\n",
        "\n",
        "def fetch_wikipedia_articles(query, num_results=3):\n",
        "    \"\"\"\n",
        "    Fetches relevant articles from Wikipedia based on a given query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query for Wikipedia articles.\n",
        "        num_results (int): The maximum number of search results to process.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary contains 'title', 'url',\n",
        "              and 'content' for a fetched Wikipedia article.\n",
        "    \"\"\"\n",
        "    articles = []\n",
        "    try:\n",
        "        # Search for potential Wikipedia page titles\n",
        "        search_titles = wikipedia.search(query, results=num_results)\n",
        "        print(f\"Found {len(search_titles)} potential Wikipedia pages for '{query}': {search_titles}\")\n",
        "\n",
        "        for title in search_titles:\n",
        "            try:\n",
        "                # Retrieve the Wikipedia page object\n",
        "                page = wikipedia.page(title, auto_suggest=False, redirect=True)\n",
        "                articles.append({\n",
        "                    'title': page.title,\n",
        "                    'url': page.url,\n",
        "                    'content': page.content\n",
        "                })\n",
        "            except wikipedia.exceptions.PageError:\n",
        "                print(f\"Page '{title}' not found. Skipping.\")\n",
        "            except wikipedia.exceptions.DisambiguationError as e:\n",
        "                print(f\"Disambiguation error for '{title}'. Options: {e.options}. Skipping.\")\n",
        "            except Exception as e:\n",
        "                print(f\"An unexpected error occurred while fetching '{title}': {e}. Skipping.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Wikipedia search for '{query}': {e}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Test the function with the provided query variable\n",
        "fetched_wikipedia_articles = fetch_wikipedia_articles(query)\n",
        "\n",
        "print(\"\\nFetched articles:\")\n",
        "if fetched_wikipedia_articles:\n",
        "    for article in fetched_wikipedia_articles:\n",
        "        print(f\"Title: {article['title']}\")\n",
        "        print(f\"URL: {article['url']}\")\n",
        "        print(\"\\n\")\n",
        "else:\n",
        "    print(\"No articles fetched.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 potential Wikipedia pages for 'Retrieval Augmented Generation': ['Retrieval-augmented generation', 'Vector database', 'Knowledge cutoff']\n",
            "\n",
            "Fetched articles:\n",
            "Title: Retrieval-augmented generation\n",
            "URL: https://en.wikipedia.org/wiki/Retrieval-augmented_generation\n",
            "\n",
            "\n",
            "Title: Vector database\n",
            "URL: https://en.wikipedia.org/wiki/Vector_database\n",
            "\n",
            "\n",
            "Title: Knowledge cutoff\n",
            "URL: https://en.wikipedia.org/wiki/Knowledge_cutoff\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7562616a",
        "outputId": "eaf952cf-31d8-4307-e637-bb29d7a33509"
      },
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Instantiate RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Create an empty list to store all chunks\n",
        "all_chunks = []\n",
        "\n",
        "# Iterate through each article and split its content into chunks\n",
        "for article in fetched_wikipedia_articles:\n",
        "    doc_chunks = text_splitter.split_text(article['content'])\n",
        "    for chunk_text in doc_chunks:\n",
        "        all_chunks.append({\n",
        "            'text': chunk_text,\n",
        "            'title': article['title'],\n",
        "            'url': article['url']\n",
        "        })\n",
        "\n",
        "# Print the total number of chunks created\n",
        "print(f\"Total number of chunks created: {len(all_chunks)}\")\n",
        "\n",
        "# Print the first few chunks to verify the splitting process\n",
        "print(\"\\nFirst 3 chunks:\")\n",
        "for i, chunk in enumerate(all_chunks[:3]):\n",
        "    print(f\"Chunk {i+1}:\")\n",
        "    print(f\"  Title: {chunk['title']}\")\n",
        "    print(f\"  URL: {chunk['url']}\")\n",
        "    print(f\"  Content: {chunk['text'][:200]}...\") # Display first 200 chars\n",
        "    print(\"---\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of chunks created: 67\n",
            "\n",
            "First 3 chunks:\n",
            "Chunk 1:\n",
            "  Title: Retrieval-augmented generation\n",
            "  URL: https://en.wikipedia.org/wiki/Retrieval-augmented_generation\n",
            "  Content: Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information from external data sources. With RAG, LLMs do not respond to u...\n",
            "---\n",
            "\n",
            "Chunk 2:\n",
            "  Title: Retrieval-augmented generation\n",
            "  URL: https://en.wikipedia.org/wiki/Retrieval-augmented_generation\n",
            "  Content: data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources....\n",
            "---\n",
            "\n",
            "Chunk 3:\n",
            "  Title: Retrieval-augmented generation\n",
            "  URL: https://en.wikipedia.org/wiki/Retrieval-augmented_generation\n",
            "  Content: RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike LLMs that rely on static training data, RAG pulls relevant text from databases, upl...\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "c15d0ee4355544368110723d979e49b2",
            "f2c5e15d72e34c538521512454bb3d12",
            "9a75d2922e6c476783b046b7e878e7fd",
            "215e1c9fb40f4815bf653a114c057a71",
            "f2c344ff4c9b40269b7efd67ff0c6cd0",
            "82c5eae736aa4d11bf76d40dca276b6e",
            "aa200df4a8a540ffb67586e713173576",
            "2e81a78e45f94e8bbb1cb20a9829954b",
            "74e1f48515434d0ea09126c7f300e724",
            "9fbed4605bea481185fdb0d55537c87a",
            "6b975547b61f4e93931395f70e518f48"
          ]
        },
        "id": "e6615030",
        "outputId": "a630395a-8ed0-468f-d372-87a0dd5f7d26"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Extract the 'text' content from each dictionary in the all_chunks list\n",
        "chunk_texts = [chunk['text'] for chunk in all_chunks]\n",
        "\n",
        "# 2. Use the loaded minilm_model to generate embeddings for all the chunk_texts\n",
        "chunk_embeddings = minilm_model.encode(chunk_texts, show_progress_bar=True)\n",
        "\n",
        "# 3. Store the generated embeddings in a variable (already done above as chunk_embeddings)\n",
        "\n",
        "# 4. Print the shape of the chunk_embeddings\n",
        "print(f\"Shape of generated embeddings: {chunk_embeddings.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c15d0ee4355544368110723d979e49b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of generated embeddings: (67, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0e38be3",
        "outputId": "22d66282-de97-44e0-858c-2b240420fb38"
      },
      "source": [
        "import faiss\n",
        "\n",
        "# 1. Get the dimensionality of the embeddings\n",
        "dimension = chunk_embeddings.shape[1]\n",
        "\n",
        "# 2. Initialize a FAISS IndexFlatL2 index\n",
        "# IndexFlatL2 calculates Euclidean distance (L2 norm)\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# 3. Add the chunk_embeddings to the FAISS index\n",
        "# FAISS requires the input to be float32\n",
        "faiss_index.add(np.array(chunk_embeddings).astype('float32'))\n",
        "\n",
        "# 4. Print the number of vectors in the index to confirm it has been populated correctly\n",
        "print(f\"Number of vectors in the FAISS index: {faiss_index.ntotal}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of vectors in the FAISS index: 67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9ade43f",
        "outputId": "b9691864-a5b0-4e95-984f-229df010f8bd"
      },
      "source": [
        "def retrieve_chunks(query, k=3):\n",
        "    \"\"\"\n",
        "    Retrieves the top-k most relevant text chunks for a given query using the FAISS index.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query string.\n",
        "        k (int): The number of top relevant chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary represents a relevant text chunk\n",
        "              including its text, title, and URL.\n",
        "    \"\"\"\n",
        "    # 1. Convert the query into an embedding using the MiniLM model\n",
        "    query_embedding = minilm_model.encode([query])\n",
        "\n",
        "    # 2. Ensure the query embedding is float32 and 2D array for FAISS\n",
        "    query_embedding = np.array(query_embedding).astype('float32')\n",
        "\n",
        "    # 3. Utilize faiss_index.search() to find the k most similar embeddings\n",
        "    # D: distances, I: indices of the nearest neighbors\n",
        "    distances, indices = faiss_index.search(query_embedding, k)\n",
        "\n",
        "    # 4. Access and compile the corresponding text chunks from the all_chunks list\n",
        "    relevant_chunks = []\n",
        "    for i in indices[0]: # indices[0] because query_embedding is a 2D array (batch of 1)\n",
        "        relevant_chunks.append(all_chunks[i])\n",
        "\n",
        "    return relevant_chunks\n",
        "\n",
        "# Test the function with an example query\n",
        "example_query = query # Using the predefined 'query' variable\n",
        "retrieved_k = 3\n",
        "\n",
        "print(f\"\\nRetrieving top {retrieved_k} chunks for query: '{example_query}'\")\n",
        "retrieved_chunks = retrieve_chunks(example_query, k=retrieved_k)\n",
        "\n",
        "if retrieved_chunks:\n",
        "    print(f\"Found {len(retrieved_chunks)} relevant chunks:\")\n",
        "    for i, chunk in enumerate(retrieved_chunks):\n",
        "        print(f\"Chunk {i+1} (Title: {chunk['title']}):\")\n",
        "        print(f\"  Content: {chunk['text'][:300]}...\") # Display first 300 chars\n",
        "        print(f\"  URL: {chunk['url']}\")\n",
        "        print(\"---\")\n",
        "else:\n",
        "    print(\"No relevant chunks found.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Retrieving top 3 chunks for query: 'Retrieval Augmented Generation'\n",
            "Found 3 relevant chunks:\n",
            "Chunk 1 (Title: Retrieval-augmented generation):\n",
            "  Content: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. Ars Technica notes that \"when new information becomes available, rather than hav...\n",
            "  URL: https://en.wikipedia.org/wiki/Retrieval-augmented_generation\n",
            "---\n",
            "Chunk 2 (Title: Knowledge cutoff):\n",
            "  Content: == Attempts to overcome knowledge cutoffs ==\n",
            "\n",
            "\n",
            "=== Retrieval-augmented generation ===...\n",
            "  URL: https://en.wikipedia.org/wiki/Knowledge_cutoff\n",
            "---\n",
            "Chunk 3 (Title: Vector database):\n",
            "  Content: recommendations engines, object detection, and retrieval-augmented generation (RAG)....\n",
            "  URL: https://en.wikipedia.org/wiki/Vector_database\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a714601",
        "outputId": "e1e4bfb4-1558-471d-b1f8-f517004155a5"
      },
      "source": [
        "import torch\n",
        "\n",
        "def generate_answer(query, retrieved_chunks):\n",
        "    \"\"\"\n",
        "    Generates a grounded answer to the user's query using the FLAN-T5-small model\n",
        "    and the provided retrieved chunks as context.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query string.\n",
        "        retrieved_chunks (list): A list of dictionaries, where each dictionary\n",
        "                                 represents a relevant text chunk.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer string.\n",
        "    \"\"\"\n",
        "    # 1. Combine retrieved chunks to form the context\n",
        "    context = \" \".join([chunk['text'] for chunk in retrieved_chunks])\n",
        "\n",
        "    # 2. Construct the prompt for the FLAN-T5 model\n",
        "    # Format: 'Context: [chunk1_text] [chunk2_text] ... Question: [user_query]'\n",
        "    prompt = f\"Context: {context} Question: {query}\"\n",
        "\n",
        "    # 3. Use the flan_tokenizer to tokenize the constructed prompt\n",
        "    inputs = flan_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # 4. Pass the tokenized input to the flan_model to generate an answer\n",
        "    # Using appropriate generation parameters\n",
        "    outputs = flan_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=150, # Maximum number of tokens to generate\n",
        "        do_sample=True,     # Use sampling\n",
        "        top_k=50,           # Consider top 50 tokens for sampling\n",
        "        top_p=0.95,         # Nucleus sampling\n",
        "        temperature=0.7,    # Sampling temperature\n",
        "        num_return_sequences=1 # Only generate one sequence\n",
        "    )\n",
        "\n",
        "    # 5. Decode the generated token IDs back into a human-readable string\n",
        "    answer = flan_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Test the generate_answer function\n",
        "print(f\"\\nGenerating answer for query: '{example_query}'\")\n",
        "final_answer = generate_answer(example_query, retrieved_chunks)\n",
        "\n",
        "print(\"\\nGenerated Answer:\")\n",
        "print(final_answer)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating answer for query: 'Retrieval Augmented Generation'\n",
            "\n",
            "Generated Answer:\n",
            "explains the main purpose of augmentations\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c15d0ee4355544368110723d979e49b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2c5e15d72e34c538521512454bb3d12",
              "IPY_MODEL_9a75d2922e6c476783b046b7e878e7fd",
              "IPY_MODEL_215e1c9fb40f4815bf653a114c057a71"
            ],
            "layout": "IPY_MODEL_f2c344ff4c9b40269b7efd67ff0c6cd0"
          }
        },
        "f2c5e15d72e34c538521512454bb3d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82c5eae736aa4d11bf76d40dca276b6e",
            "placeholder": "​",
            "style": "IPY_MODEL_aa200df4a8a540ffb67586e713173576",
            "value": "Batches: 100%"
          }
        },
        "9a75d2922e6c476783b046b7e878e7fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e81a78e45f94e8bbb1cb20a9829954b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74e1f48515434d0ea09126c7f300e724",
            "value": 3
          }
        },
        "215e1c9fb40f4815bf653a114c057a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fbed4605bea481185fdb0d55537c87a",
            "placeholder": "​",
            "style": "IPY_MODEL_6b975547b61f4e93931395f70e518f48",
            "value": " 3/3 [00:03&lt;00:00,  1.42s/it]"
          }
        },
        "f2c344ff4c9b40269b7efd67ff0c6cd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82c5eae736aa4d11bf76d40dca276b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa200df4a8a540ffb67586e713173576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e81a78e45f94e8bbb1cb20a9829954b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74e1f48515434d0ea09126c7f300e724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fbed4605bea481185fdb0d55537c87a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b975547b61f4e93931395f70e518f48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}